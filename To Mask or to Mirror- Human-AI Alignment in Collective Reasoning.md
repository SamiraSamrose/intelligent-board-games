This paper investigates whether large language models (LLMs) can accurately simulate human collective decision-making by examining leader election dynamics in the "Lost at Sea" task with 748 human participants and matched LLM simulations (Gemini, GPT, Claude). The study manipulated demographic visibility by randomly assigning groups to either identified conditions (with visible names, avatars, gender) or pseudonymous conditions (with gender-neutral animal aliases), finding that humans exhibited gender bias in leader selection that decreased under pseudonymity. The researchers discovered a "mask-mirror" tension in LLM behavior: Gemini and GPT closely mirrored human biases including suboptimal leader choices when given demographic information, while Claude masked these biases by selecting more optimal leaders but with lower alignment to actual human decisions. When demographic context was removed entirely, all models defaulted to male-aligned choices and lost alignment with human outcomes, demonstrating that identity cues are essential for social simulation. The findings reveal that human-AI alignment in collective reasoning is context-dependent and model-specific, highlighting the need to distinguish between descriptive alignment (matching human behavior including biases) and normative alignment (achieving better outcomes) when deploying LLMs in social settings.