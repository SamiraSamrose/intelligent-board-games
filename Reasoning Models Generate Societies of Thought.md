This paper demonstrates that advanced reasoning models like DeepSeek-R1 and QwQ-32B improve problem-solving not merely through longer chains of thought, but by simulating internal "societies of thought"â€”multi-agent-like conversations between diverse cognitive perspectives with distinct personalities and expertise. Through analysis of reasoning traces, the researchers found these models exhibit significantly more conversational behaviors (question-answering, perspective shifts, conflicts, and reconciliation) and socio-emotional roles compared to standard instruction-tuned models, with this diversity directly contributing to reasoning accuracy. Mechanistic interpretability experiments showed that steering conversational features in the model's activation space doubles reasoning accuracy and activates more diverse personality- and expertise-related features. Reinforcement learning experiments revealed that models spontaneously develop these conversational patterns when rewarded solely for accuracy, and that pre-training with conversational scaffolding accelerates reasoning improvement compared to monologue-style reasoning. The findings suggest that effective reasoning in AI systems emerges from structured interactions among differentiated internal perspectives, mirroring principles of collective intelligence in human groups.
